# -*- coding: utf-8 -*-
"""OS Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18W3DmJuKFgTcHBuGKd_2GKolmocHF1Zg
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# Configuration parameters
config = {
    'cpu_freq': 2e9, 'cores_per_socket': 4, 'num_sockets': 4,
    'shared_cache': 1e6, 'local_dram_latency': 125,
    'remote_dram_latency': [250, 375, 500], 'local_cache_latency': 50,
    'remote_cache_latency': [100, 175, 250], 'scheduling_quanta': 16,
    'cache_line_size': 64, 'cache_affinity_penalty': 1024 * 250
}

# Synthetic workload generation
def generate_synthetic_workload(phase_type='Synth1'):
    np.random.seed()
    n_threads = config['num_sockets'] * config['cores_per_socket']

    if phase_type == 'Synth1':
        quanta_patterns = [1] * config['scheduling_quanta']
    elif phase_type == 'Synth2':
        quanta_patterns = [1]*8 + [2]*8
    else:  # Synth3
        quanta_patterns = [1]*4 + [2]*4 + [3]*4 + [4]*4

    c2c_transfers = {}
    dram_accesses = {}
    for phase in set(quanta_patterns):
        base_pattern = np.random.randint(1, 10, size=(n_threads, n_threads))
        c2c_matrix = (base_pattern + base_pattern.T) / 2
        np.fill_diagonal(c2c_matrix, 0)
        dram_matrix = np.random.randint(1000, 10000, size=(n_threads, config['num_sockets']))
        c2c_transfers[phase] = c2c_matrix
        dram_accesses[phase] = dram_matrix

    return {
        'phase_type': phase_type,
        'quanta_patterns': quanta_patterns,
        'c2c_transfers': c2c_transfers,
        'dram_accesses': dram_accesses
    }

# Scheduling algorithms
def algorithm1_c2c(c2c_matrix):
    n_threads = c2c_matrix.shape[0]
    threads = list(range(n_threads))
    groups = []
    while threads:
        max_pairs = [(i, np.argmax(c2c_matrix[i])) for i in threads]
        max_values = [c2c_matrix[i][j] for i, j in max_pairs]
        p_idx = np.argmax(max_values)
        p, j = max_pairs[p_idx]
        top_indices = np.argsort(c2c_matrix[p])[::-1][:4]
        group = [p] + [x for x in top_indices if x != p][:3]
        groups.append(group)
        threads = [t for t in threads if t not in group]
    return groups

def algorithm2_c2c(c2c_matrix):
    n_threads = c2c_matrix.shape[0]
    threads = list(range(n_threads))
    groups = []
    pairs = [(i, j, c2c_matrix[i][j]) for i in range(n_threads) for j in range(i+1, n_threads)]
    pairs.sort(key=lambda x: -x[2])

    while threads:
        if not pairs:
            groups.append(threads[:4])
            threads = threads[4:]
            continue

        i, j, _ = pairs.pop(0)
        group = {i, j}
        for k, l, val in pairs:
            if (k in group or l in group) and len(group) < 4:
                group.add(k if k not in group else l)
            if len(group) == 4:
                break

        pairs = [p for p in pairs if p[0] not in group and p[1] not in group]
        groups.append(list(group))
        threads = [t for t in threads if t not in group]
    return groups

def algorithm2_dram(groups, dram_accesses):
    n_groups = len(groups)
    assignments = {}
    available_nodes = list(range(config['num_sockets']))

    for node in range(config['num_sockets']):
        if not available_nodes: break
        best_group, best_access = None, -1
        for g_idx, group in enumerate(groups):
            if g_idx not in assignments:
                total_access = sum(dram_accesses[t][node] for t in group)
                if total_access > best_access:
                    best_access, best_group = total_access, g_idx
        if best_group is not None:
            assignments[best_group] = node
            available_nodes.remove(node)

    for g_idx, group in enumerate(groups):
        if g_idx not in assignments:
            assignments[g_idx] = available_nodes[0] if available_nodes else 0
    return assignments

# Clustered versions
def algorithm1_c2c_with_clustering(c2c_matrix, cluster_labels):
    threads = list(range(len(c2c_matrix)))
    groups = []
    cluster_groups = {}
    for t in threads:
        cluster = cluster_labels[t]
        cluster_groups.setdefault(cluster, []).append(t)

    for cluster, cluster_threads in cluster_groups.items():
        sub_matrix = c2c_matrix[np.ix_(cluster_threads, cluster_threads)]
        for group in algorithm1_c2c(sub_matrix):
            groups.append([cluster_threads[i] for i in group])
    return groups

def algorithm2_c2c_with_clustering(c2c_matrix, cluster_labels):
    threads = list(range(len(c2c_matrix)))
    groups = []
    cluster_groups = {}
    for t in threads:
        cluster = cluster_labels[t]
        cluster_groups.setdefault(cluster, []).append(t)

    for cluster, cluster_threads in cluster_groups.items():
        sub_matrix = c2c_matrix[np.ix_(cluster_threads, cluster_threads)]
        for group in algorithm2_c2c(sub_matrix):
            groups.append([cluster_threads[i] for i in group])
    return groups

# Latency calculation
def calculate_latency(groups, group_assignments, c2c_matrix, dram_accesses,
                     remote_cache_latency, remote_dram_latency, consider_affinity=False):
    total_latency = 0
    thread_to_group = {}
    for g_idx, group in enumerate(groups):
        for thread in group:
            thread_to_group[thread] = g_idx

    # Cache-to-cache transfers
    for i in range(len(c2c_matrix)):
        for j in range(i+1, len(c2c_matrix)):
            i_group = thread_to_group[i]
            j_group = thread_to_group[j]
            latency = config['local_cache_latency'] if (
                i_group == j_group or
                group_assignments.get(i_group, -1) == group_assignments.get(j_group, -2)
            ) else remote_cache_latency
            total_latency += c2c_matrix[i][j] * latency

    # DRAM accesses
    for thread in range(len(dram_accesses)):
        group = thread_to_group[thread]
        node = group_assignments.get(group, thread % config['num_sockets'])
        total_latency += dram_accesses[thread][node] * config['local_dram_latency']
        for remote_node in range(config['num_sockets']):
            if remote_node != node:
                total_latency += dram_accesses[thread][remote_node] * remote_dram_latency

    if consider_affinity:
        total_latency += len(thread_to_group) * config['cache_affinity_penalty'] / 4
    return total_latency

# Clustering features
def create_clustering_features(workload):
    features = []
    thread_features = {t: [] for t in range(16)}
    for phase in workload['c2c_transfers']:
        c2c_matrix = workload['c2c_transfers'][phase]
        dram_matrix = workload['dram_accesses'][phase]
        for t in range(16):
            thread_features[t].extend([
                np.sum(c2c_matrix[t]), np.max(c2c_matrix[t]), np.mean(c2c_matrix[t]),
                dram_matrix[t][t % config['num_sockets']],
                np.sum(dram_matrix[t]) - dram_matrix[t][t % config['num_sockets']]
            ])
    return np.array([thread_features[t] for t in range(16)])

# Generate workloads
workloads = {
    'Static': generate_synthetic_workload('Synth1'),
    'Two-Phase': generate_synthetic_workload('Synth2'),
    'Four-Phase': generate_synthetic_workload('Synth3')
}

# Apply clustering
for w_name, workload in workloads.items():
    features = create_clustering_features(workload)
    workload['cluster_labels'] = KMeans(n_clusters=2, random_state=42).fit_predict(features)

# Run experiments
results = []
hybrid_data = []

for w_name, workload in workloads.items():
    for quantum in range(config['scheduling_quanta']):
        phase = workload['quanta_patterns'][quantum]
        c2c_matrix = workload['c2c_transfers'][phase]
        dram_matrix = workload['dram_accesses'][phase]
        cluster_labels = workload['cluster_labels']

        # Baseline
        random_groups = [list(range(i, i+4)) for i in range(0, 16, 4)]
        no_opt_latency = calculate_latency(
            random_groups, {i:i for i in range(4)}, c2c_matrix, dram_matrix,
            config['remote_cache_latency'][0], config['remote_dram_latency'][0]
        )

        # Original algorithms
        groups_algo1 = algorithm1_c2c(c2c_matrix)
        algo1_latency = calculate_latency(
            groups_algo1, algorithm2_dram(groups_algo1, dram_matrix),
            c2c_matrix, dram_matrix,
            config['remote_cache_latency'][0], config['remote_dram_latency'][0],
            consider_affinity=True
        )

        groups_algo2 = algorithm2_c2c(c2c_matrix)
        algo2_latency = calculate_latency(
            groups_algo2, algorithm2_dram(groups_algo2, dram_matrix),
            c2c_matrix, dram_matrix,
            config['remote_cache_latency'][0], config['remote_dram_latency'][0],
            consider_affinity=True
        )

        # Clustered versions
        groups_algo1_cluster = algorithm1_c2c_with_clustering(c2c_matrix, cluster_labels)
        algo1_cluster_latency = calculate_latency(
            groups_algo1_cluster, algorithm2_dram(groups_algo1_cluster, dram_matrix),
            c2c_matrix, dram_matrix,
            config['remote_cache_latency'][0], config['remote_dram_latency'][0],
            consider_affinity=True
        )

        groups_algo2_cluster = algorithm2_c2c_with_clustering(c2c_matrix, cluster_labels)
        algo2_cluster_latency = calculate_latency(
            groups_algo2_cluster, algorithm2_dram(groups_algo2_cluster, dram_matrix),
            c2c_matrix, dram_matrix,
            config['remote_cache_latency'][0], config['remote_dram_latency'][0],
            consider_affinity=True
        )

        # Store results
        results.append({
            'workload': w_name, 'quantum': quantum,
            'no_opt': no_opt_latency,
            'algo1': algo1_latency, 'algo2': algo2_latency,
            'algo1_cluster': algo1_cluster_latency,
            'algo2_cluster': algo2_cluster_latency
        })

        # Prepare hybrid training data
        hybrid_data.append([
            np.sum(c2c_matrix), np.mean(c2c_matrix), np.std(c2c_matrix),
            np.sum(dram_matrix), np.sum(np.diag(dram_matrix)),
            np.sum(dram_matrix) - np.sum(np.diag(dram_matrix)),
            1 if algo1_latency < algo2_latency else 0
        ])

# Train hybrid model
X = np.array([x[:-1] for x in hybrid_data])
y = np.array([x[-1] for x in hybrid_data])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
hybrid_model = LinearRegression().fit(X_train, y_train)

# Hybrid scheduler
hybrid_results = []
results_df = pd.DataFrame(results)

for w_name, workload in workloads.items():
    for quantum in range(config['scheduling_quanta']):
        phase = workload['quanta_patterns'][quantum]
        c2c_matrix = workload['c2c_transfers'][phase]
        dram_matrix = workload['dram_accesses'][phase]
        cluster_labels = workload['cluster_labels']

        # Predict best algorithm
        features = np.array([[
            np.sum(c2c_matrix), np.mean(c2c_matrix), np.std(c2c_matrix),
            np.sum(dram_matrix), np.sum(np.diag(dram_matrix)),
            np.sum(dram_matrix) - np.sum(np.diag(dram_matrix))
        ]])
        pred = hybrid_model.predict(features)[0]
        use_algo1 = pred > 0.5

        # Run selected algorithm
        if use_algo1:
            groups = algorithm1_c2c_with_clustering(c2c_matrix, cluster_labels)
            latency = calculate_latency(
                groups, algorithm2_dram(groups, dram_matrix),
                c2c_matrix, dram_matrix,
                config['remote_cache_latency'][0], config['remote_dram_latency'][0],
                consider_affinity=True
            )
        else:
            groups = algorithm2_c2c_with_clustering(c2c_matrix, cluster_labels)
            latency = calculate_latency(
                groups, algorithm2_dram(groups, dram_matrix),
                c2c_matrix, dram_matrix,
                config['remote_cache_latency'][0], config['remote_dram_latency'][0],
                consider_affinity=True
            )

        # Get baseline
        baseline_match = results_df[(results_df['workload'] == w_name) &
                                 (results_df['quantum'] == quantum)]
        if not baseline_match.empty:
            no_opt = baseline_match['no_opt'].values[0]
            improvement = (no_opt - latency) / no_opt * 100
        else:
            random_groups = [list(range(i, i+4)) for i in range(0, 16, 4)]
            no_opt = calculate_latency(
                random_groups, {i:i for i in range(4)},
                c2c_matrix, dram_matrix,
                config['remote_cache_latency'][0], config['remote_dram_latency'][0]
            )
            improvement = (no_opt - latency) / no_opt * 100

        hybrid_results.append({
            'workload': w_name,
            'quantum': quantum,
            'latency': latency,
            'improvement': improvement,
            'algorithm': 'Algorithm 1' if use_algo1 else 'Algorithm 2'
        })

# Process results
def calculate_improvement(new, base):
    return (base - new) / base * 100

agg_results = results_df.groupby('workload').agg({
    'no_opt': 'mean',
    'algo1': 'mean',
    'algo2': 'mean',
    'algo1_cluster': 'mean',
    'algo2_cluster': 'mean'
}).reset_index()

for col in ['algo1', 'algo2', 'algo1_cluster', 'algo2_cluster']:
    agg_results[f'{col}_improvement'] = calculate_improvement(agg_results[col], agg_results['no_opt'])

hybrid_df = pd.DataFrame(hybrid_results)
hybrid_agg = hybrid_df.groupby('workload').agg({
    'latency': 'mean',
    'improvement': 'mean'
}).reset_index()

# Visualization
plt.figure(figsize=(16, 6))
bar_width = 0.2
index = np.arange(len(agg_results))

# Original vs Clustered
plt.subplot(1, 2, 1)
plt.bar(index - bar_width*1.5, agg_results['algo1_improvement'],
        bar_width, label='Original Algo1', color='skyblue')
plt.bar(index - bar_width/2, agg_results['algo1_cluster_improvement'],
        bar_width, label='Clustered Algo1', color='royalblue')
plt.bar(index + bar_width/2, agg_results['algo2_improvement'],
        bar_width, label='Original Algo2', color='lightcoral')
plt.bar(index + bar_width*1.5, agg_results['algo2_cluster_improvement'],
        bar_width, label='Clustered Algo2', color='firebrick')

plt.xlabel('Workload Type')
plt.ylabel('% Improvement in Latency')
plt.title('Original vs Clustered Algorithms')
plt.xticks(index, agg_results['workload'])
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Hybrid comparison
plt.subplot(1, 2, 2)
plt.bar(index - bar_width, agg_results['algo1_cluster_improvement'],
        bar_width, label='Clustered Algo1', color='royalblue')
plt.bar(index, agg_results['algo2_cluster_improvement'],
        bar_width, label='Clustered Algo2', color='firebrick')
plt.bar(index + bar_width, hybrid_agg['improvement'],
        bar_width, label='Hybrid Scheduler', color='mediumseagreen')

plt.xlabel('Workload Type')
plt.ylabel('% Improvement in Latency')
plt.title('Hybrid vs Clustered Algorithms')
plt.xticks(index, agg_results['workload'])
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# Algorithm selection frequency
algorithm_choice = hybrid_df.groupby(['workload', 'algorithm']).size().unstack()
algorithm_choice.plot(kind='bar', stacked=True, figsize=(10, 6),
                     color=['skyblue', 'lightcoral'])
plt.title('Hybrid Scheduler Algorithm Selection Frequency')
plt.ylabel('Number of Quanta')
plt.xlabel('Workload Type')
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()